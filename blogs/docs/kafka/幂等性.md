[生产端幂等性](#生产端幂等性)  
[消费端幂等性](#消费端幂等性) 

### 生产端幂等性
Kafka引入了Producer ID（即PID）和Sequence Number。  
每个新的Producer在初始化的时候会被分配一个唯一的PID，该PID对用户完全透明而不会暴露给用户。  
对于每个PID，该Producer发送数据的每个<Topic, Partition>都对应一个从0开始单调递增的Sequence Number。

类似地，Broker端也会为每个<PID, Topic, Partition>维护一个序号，并且每次Commit一条消息时将其对应序号递增。  
对于接收的每条消息，如果其序号比Broker维护的序号（即最后一次Commit的消息的序号）大一，则Broker会接受它，否则将其丢弃：
- 如果消息序号比Broker维护的序号大一以上，说明中间有数据尚未写入，也即乱序，此时Broker拒绝该消息，Producer抛出InvalidSequenceNumber
- 如果消息序号小于等于Broker维护的序号，说明该消息已被保存，即为重复消息，Broker直接丢弃该消息，Producer抛出DuplicateSequenceNumber

上述设计解决了0.11.0.0之前版本中的两个问题：
- Broker保存消息后，发送ACK前宕机，Producer认为消息未发送成功并重试，造成数据重复
- 前一条消息发送失败，后一条消息发送成功，前一条消息重试后成功，造成数据乱序。

### 消费端幂等性
Kafka有个offset的概念，就是每个消息都有一个序号(offset)。consumer消费了数据之后，每隔一段时间(定时定期)，会把自己消费过的消息的offset提交一下，表示已经消费过了。

但是凡事总有意外，就是有时候重启系统，会导致consumer有些消息处理了，但是没来得及提交offset。重启之后，少数消息会再次消费一次。

因此需要结合具体的业务来保证消费的幂等性：  
比如写数据库库，先根据主键查一下；  
比如写Redis，没有问题，因为每次都是set，天然幂等性；  
比如让生产者生产消息时加一个全局唯一的id。然后消费到了后，先根据这个id去Redis/数据库里查一下，如果没有消费过才继续处理；  
比如基于数据库的唯一索引来保证重复数据不会重复插入。因为有唯一索引约束了，重复数据插入只会报错。


